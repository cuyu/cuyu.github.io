---
layout: post
title: "About Deep Q Network"
category: Machine Learning
tags: [neural network, reinforcement learning]
date: 2018-11-16
---

1. DQN是Q learning和Neural network结合的产物，而Q learning是强化学习经典的算法。Q learning的思路其实不复杂：假设在某一步骤时有N个选择，根据状态表（关于这个概念后面会再聊），选择了option A，进入到下一个步骤，继续根据状态表选择了option B，以此类推，直到最后一步的选择得到了结果，如果这个结果正是我们想得到的，则我们会给予一个奖励，怎么个奖励法？其实就是更新状态表，把之前的各个状态下对应的选择的权重进行提升。否则，如果结果不是我们想要的，则进行惩罚：当然也是更新状态表。所谓状态表，是这样一张表：它的每行都是一个状态，每列是对应该状态的各种选择，表中的值则是选择所对应的权重（也可以理解为做各个选择所期望得到的奖励）：

   |       | option A | option B | option C | option D |
   | ------ | -------- | -------- | -------- | -------- |
   | 状态一 | -0.12 | 0.28 | 0      | 0       |
   | 状态二 | 0.33 | 0       | -0.21 | 0       |
   | 状态三 | 0       | -0.02 | 0.31 | 0.54 |

   这里的状态根据所解决的问题不同而不同，比如说走迷宫的问题，那么走迷宫的人的坐标就是状态，而上下左右四个方向则是选项。需要注意的是，并不是每个状态下都会对应所有的选项的，比如在迷宫底部，向下这个选项就是不存在的（或者说它的权重无限低）。

2. Q learning中的Q是Quality的缩写，就是状态表中的权重也可以认为是Q值（选择的质量）。

3. Q learning如果仅仅在最后一步达到目标后再去更新状态表，其实是有一点不合适的，假设我们一共做了n次选择，我们当然可以给第n次选择奖励1，第n-1次选择奖励0.9，以此类推。但是这样往往最终得到的并不是最优解，可能我们可以少做几次选择也能达到目标，但由于这种奖励方式，导致多余的那几次选择都得到了奖励。所以，实际的Q learning是这样“发放奖励”的：

   - 如果当前的选择之后的状态即是我们所想要达到的目标状态，那么我们直接给予该选择一个固定的奖励；
   - 如果当前的选择之后的状态并非是最终的目标状态，那么我们使用该选择之后的状态下的所有选项中的最高权重（事实上这个拥有最高权重的选项就是下一步要做的选择），以一定比例给予当前的选择。

   所以，Q learning是每一次选择结束都会去更新状态表的。

4. Q learning训练的过程就是一个不断去更新状态表的过程，做预测时，状态表就是决策表。对于复杂的问题，Q learning有这么些缺点：

   - 状态难以定义
   - 状态表存储成本高
   - “奖惩”力度难以把控

   并且，为了学习解决一个问题需要数以万次的尝试，这离人类的智能相差很远，这也是目前强化学习（不仅是Q learning）被人所病垢比较多的一点。

5. 试想下，在Q learning的算法中，神经网络可以做些什么？首先，做选择可以交由神经网络来做，假设我们一共有N种选择，构造一个输出层含有N个神经元的网络即可，它的输入当然就是当前时刻的状态，这样我们就不用再创建上面那张状态表了；其次，“奖惩”力度也可以由神经网络来决定，它的输入为下一时刻的状态（即做出当前选择之后的状态），输出为预测的下一个状态下各个选择的权重（然后同样用预测的之后的所有选择的最高权重来给予奖励）。前者在DQN中称之为evaluate network，后者为target network。
   <!--break-->

6. DQN中的evaluate network和target network的输入都是一个状态，只不过后者输入的状态是前者的输出所决定的，而输出都是所有选择的权重。因此，直观上来说，这两个网络的结构应该是一样的，事实上也是如此。

7. 既然evaluate network和target network目的是一样的，都是根据状态来预测选择，那么为什么还需要分别训练两个网络呢？主要是考虑到稳定性（试想如果仍通过evaluate network来确定奖励，刚开始训练时，evaluate network并不知道哪些选择会更优，它的输出有很多噪声，如果我们选择了一个错误的输出给予了奖励，并且随着每次的选择奖励还在不断变多，那训练必然走了很多弯路；而根据target network来给予奖励，至少奖励不会变多，之后的训练可以比较容易地把错误修正回来；原始的Q learning算法没有这个问题是因为它不存在噪声的问题，没有随机性，每一次选择都是严格意义上的最优选择，当然代价是一张巨大的状态表）：

   >  Imagine one of the data points (at `S, A, R, S'`) causes a currently poor over-estimate for `Q(S', A')` to get worse. Maybe `S', A'` has not even been visited yet, or the value of `R` seen so far is higher than average, just by chance. If a sample of `(S,A)` cropped up multiple times in experience replay, it would get worse again each time, because the update to `Q(S,A)` is based on `R + max_a Q(S',a)`. Fixing the target network limits the damage that such over-estimates can do, giving the learning network time to converge and lose more of its initial bias.

8. 实际训练时，我们只会去训练evaluate network，target network一开始会使用和evaluate network同样的参数（权重和偏置），在训练过固定的轮次之后（比如说每1000次），再使用当前训练的evaluate network中的参数来更新target network中的参数。即我们从头到尾都只是在训练evaluate network而已，target network其实只是evaluate network在训练的不同阶段的快照。这样我们就保证了“奖惩”的标准是相对稳定的。

9. DQN另外一个比较创新的地方在于，它使用了一个缓存（也可以理解为一个pool）来存储最近的一些状态（`S, A, R, S'`），训练时随机的在其中选取指定batch size的样本来进行训练。（按照原始的Q learning算法，可以认为训练的batch size是1。）

10. double DQN是基于DQN的一个改进算法，改进的部分在于对“奖惩”的力度做了一些修正。我们知道Q learning中奖励的大小取决于状态表中下一次状态下的最大Q值，而DQN中奖励的大小取决于target network在下一次状态下得到的最大输出（也可以理解为Q值）。在double DQN中，奖励的大小则是在evaluate network中得到的，但并不是evaluate network在下一次状态下输出的最大值，而是利用了target network来获取一个索引，然后使用了evaluate network的输出在该索引处的值。当然，这个索引就是target network的输出最大值所在的位置。

11. double DQN这样修正奖励因子的好处显而易见：在不失去稳定性的同时（训练时，同样状态下，target network没变，即索引没变，那么evaluate network的输出在同样的位置处变化也不会太大），还利用了最新的状态表中的Q值（evaluate network可以理解一个黑盒的状态表吧）来进行训练。

12. dueling DQN是基于DQN的另一个改进算法，改进的部分在网络的结构，这里懒得赘述了。

13. 以上，感觉如果说DQN相比Q learning是一个质变的话，其他改进版DQN则没那么大变化，而且改进也都能很好的解释得通，这让我想起了所有特征都靠人工来提取的时代，但事实证明，结果更好的方法往往是我们难以解释的方法（或许是因为能经过我们解释的往往是一些“精炼”过的东西，我们就有点像是翻译家，即使翻译水平再高，对于机器学习算法而言，获取的二手信息仍然是有损失的）。也许神经网络发展的最终形态，是一些大的神经网络下面挂载了许许多多子网络，且子网络的结构也可以随训练而变化，一定要有一个名称的话，那就叫神经网络森林吧😆。



Reference

1. https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4
2. https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow
3. https://ai.stackexchange.com/questions/6982/why-does-dqn-require-two-different-networks
4. https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682