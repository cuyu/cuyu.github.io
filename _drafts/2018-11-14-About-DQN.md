---
layout: post
title: "About Deep Q Network"
category: Machine Learning
tags: [neural network, reinforcement learning]
date: 2018-11-14
---

1. DQN是Q learning和Neural network结合的产物，而Q learning是强化学习经典的算法。Q learning的思路其实不复杂：假设在某一步骤时有N个选择，根据状态表（关于这个概念后面会再聊），选择了option A，进入到下一个步骤，继续根据状态表选择了option B，以此类推，直到最后一步的选择得到了结果，如果这个结果正是我们想得到的，则我们会给予一个奖励，怎么个奖励法？其实就是更新状态表，把之前的各个状态下对应的选择的权重进行提升。否则，如果结果不是我们想要的，则进行惩罚：当然也是更新状态表。所谓状态表，是这样一张表：它的每行都是一个状态，每列是对应该状态的各种选择，表中的值则是选择所对应的权重（也可以理解为做各个选择所期望得到的奖励）：

   |       | option A | option B | option C | option D |
   | ------ | -------- | -------- | -------- | -------- |
   | 状态一 | -0.12 | 0.28 | 0      | 0       |
   | 状态二 | 0.33 | 0       | -0.21 | 0       |
   | 状态三 | 0       | -0.02 | 0.31 | 0.54 |

   这里的状态根据所解决的问题不同而不同，比如说走迷宫的问题，那么走迷宫的人的坐标就是状态，而上下左右四个方向则是选项。需要注意的是，并不是每个状态下都会对应所有的选项的，比如在迷宫底部，向下这个选项就是不存在的（或者说它的权重无限低）。

2. Q learning如果仅仅在最后一步达到目标后再去更新状态表，其实是有一点不合适的，假设我们一共做了n次选择，我们当然可以给第n次选择奖励1，第n-1次选择奖励0.9，以此类推。但是这样往往最终得到的并不是最优解，可能我们可以少做几次选择也能达到目标，但由于这种奖励方式，导致多余的那几次选择都得到了奖励。所以，实际的Q learning是这样“发放奖励”的：

   - 如果当前的选择之后的状态即是我们所想要达到的目标状态，那么我们直接给予该选择一个固定的奖励；
   - 如果当前的选择之后的状态并非是最终的目标状态，那么我们使用该选择之后的状态下的所有选项中的最高权重（事实上这个拥有最高权重的选项就是下一步要做的选择），以一定比例给予当前的选择。

   所以，Q learning是每一次选择结束都会去更新状态表的。

3. Q learning训练的过程就是一个不断去更新状态表的过程，做预测时，状态表就是决策表。对于复杂的问题，Q learning有这么些缺点：

   - 状态难以定义
   - 状态表存储成本高
   - “奖惩”力度难以确定

   并且，为了学习解决一个问题需要数以万次的尝试，这离人类的智能相差很远，这也是目前强化学习（不仅是Q learning）被人所病垢比较多的一点。

4. 试想下，在Q learning的算法中，神经网络可以做些什么？首先，做选择可以交由神经网络来做，假设我们一共有N种选择，构造一个输出层含有N个神经元的网络即可，它的输入当然就是当前时刻的状态，这样我们就不用再维护上面那张状态表了；其次，“奖惩”力度也可以由神经网络来决定，。前者在DQN中称之为evaluate network，后者为target network。
   <!--break-->

5. 