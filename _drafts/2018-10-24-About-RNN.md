---
layout: post
title: "About Recurrent Neural Network"
category: Machine Learning
tags: [neural network]
date: 2018-10-24
---

1. RNN和普通的神经网络的区别在于，它中间隐层的状态不仅取决于输入值，还取决于隐层之前的状态值。即对于普通的神经网络，`S = f(W * X + b)`，其中S为隐层的状态值（可以理解为隐层神经元的输出），X为输入向量，W为隐层神经元的权重，b为偏置，f为激活函数；而对于RNN，`S1 = f(W * X + b + W_s * S0)`，其中，S1为隐层当前的状态值，S0为隐层上一个时刻的状态值，W_s为权重。

   > RNNs are called *recurrent* because they perform the same task for every element of a sequence, with the output being depended on the previous computations.

2. 因为RNN隐层上一时刻的状态值又包含了上上一时刻的状态值，所以理论上，某一时刻隐层的状态值是有其之前的所有时刻的状态值共同决定的。假设W_s是小于1的权重，那么越靠近当前时刻的状态对目前状态的影响越大，这也比较符合我们的经验认知。

   > In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps (more on this later). 

3. 如果我们把RNN中循环的网络展开来，其实很像含有多个隐层的普通神经网络，只不过每个隐层都多了一个外部输入，且所有隐层共享了权重和偏置（这一点不禁联想到CNN）。在某种程度上来说，RNN和CNN都可以算是DNN的特例，所以也不要奇怪有些事情不光CNN能做，RNN也能实现（甚至DNN也可以做到）。比如说[这个代码](https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/402_RNN_classification.py)就实现了使用RNN来对MNIST手写数字进行分类，它把图像的每一行像素作为输入，输入到一个循环了N（N为图像高度）次的RNN中，最后用一个全连接层作为输出。
   ![A recurrent neural network and the unfolding in time of the computation involved in its forward computation.](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)
   <!--break-->

4. RNN的网络结构有这么几种：
   ![img](http://karpathy.github.io/assets/rnn/diags.jpeg)

   > Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: **(1)** Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). **(2)** Sequence output (e.g. image captioning takes an image and outputs a sentence of words). **(3)** Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). **(4)** Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). **(5)** Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.

5. How to predict the 10th value if we set the length of recurrent steps to 100?
   根据下面的*条目8*可知，一种思路是一次性直接输出所有位置的预测值，

6. How to predict only one output if we use the batch more than 1 for training?
   要知道我们训练的是神经网络的参数，这些参数的维度和数目是和batch的size无关的，它们取决于输入向量的维度、神经元的数目等。因此，对于神经网络，我们给定输入的batch size，那么输出也是同样的batch size，即使在训练过程中，它（理论上）也完全是可以变化的，batch的作用仅仅在于计算损失时可以同时考虑多个样本而已。

   之所以有这个问题是因为在使用tensorflow时碰到了输入的size不匹配会报错的情况，因为tensorflow中一个model一旦定义好就是固定的了。目前tensorflow可以这样做：

   ```python
   
   ```

7. LSTM and GRU

8. How to put state of LSTM into a dense layer?
   看了几个项目都是直接把RNN的最后一次的输出作为全连接层的输入，并把全连接的输出作为最终的输出。以生成文字为例，假设我们输入的文字长度为5，因为要预测的是每个字后面一个字，所有预测的文字长度当然也是5，最终的全连接层会直接输出一个5*N的向量来表示预测的所有文字。（但感觉这样根据前面对RNN的理解，越前面的文字的预测会更加不靠谱一点？）



Reference

1. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
2. http://karpathy.github.io/2015/05/21/rnn-effectiveness/
3. 