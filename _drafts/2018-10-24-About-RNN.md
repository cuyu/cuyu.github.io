---
layout: post
title: "About Recurrent Neural Network"
category: Machine Learning
tags: [neural network]
date: 2018-10-24
---

1. RNN和普通的神经网络的区别在于，它中间隐层的状态不仅取决于输入值，还取决于隐层之前的状态值。即对于普通的神经网络，`S = f(W * X + b)`，其中S为隐层的状态值（可以理解为隐层神经元的输出），X为输入向量，W为隐层神经元的权重，b为偏置，f为激活函数；而对于RNN，`S1 = f(W * X + b + W_s * S0)`，其中，S1为隐层当前的状态值，S0为隐层上一个时刻的状态值，W_s为权重。

   > RNNs are called *recurrent* because they perform the same task for every element of a sequence, with the output being depended on the previous computations.

2. 因为RNN隐层上一时刻的状态值又包含了上上一时刻的状态值，所以理论上，某一时刻隐层的状态值是有其之前的所有时刻的状态值共同决定的。假设W_s是小于1的权重，那么越靠近当前时刻的状态对目前状态的影响越大，这也比较符合我们的经验认知。

   > In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps (more on this later). 

3. 如果我们把RNN中循环的网络展开来，其实很像含有多个隐层的普通神经网络，只不过每个隐层都多了一个外部输入，且所有隐层共享了权重和偏置（这一点不禁联想到CNN）。在某种程度上来说，RNN和CNN都可以算是DNN的特例，所以也不要奇怪有些事情不光CNN能做，RNN也能实现（甚至DNN也可以做到）。比如说[这个代码](https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/402_RNN_classification.py)就实现了使用RNN来对MNIST手写数字进行分类，它把图像的每一行像素作为输入，输入到一个循环了N（N为图像高度）次的RNN中，最后用一个全连接层作为输出。
   ![A recurrent neural network and the unfolding in time of the computation involved in its forward computation.](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)

4. 



Reference

1. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
2. 