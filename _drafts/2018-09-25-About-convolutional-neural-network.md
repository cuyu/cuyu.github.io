---
layout: post
title: "About convolutional neural network"
category: Machine Learning
tags: [neural network]
date: 2018-09-25
---

å…¶å®å…³äºCNNå¾ˆæ—©ä¹‹å‰å°±å·²ç»çŸ¥é“äº†å®ƒå¤§æ¦‚æ˜¯ä»€ä¹ˆåŸç†ï¼Œä»¥åŠå®ƒä¸ºä½•æ¯”äººä¸ºæå–çš„ç‰¹å¾ç”¨æ¥è®­ç»ƒå¾—åˆ°çš„æ•ˆæœæ›´å¥½ã€‚å¯ä»¥è¯´CNNå‡ ä¹ç»ˆç»“äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸé äººä¸ºæå–ç‰¹å¾çš„æ—¶ä»£ï¼ˆä¹Ÿå°±æ˜¯æˆ‘åˆšå¼€å§‹å­¦ä¹ è¿›å…¥è¿™é¢†åŸŸçš„æ—¶å€™ğŸ˜‚ï¼‰ã€‚

ä½†è¿™é‡Œæˆ‘æƒ³ä»å¤´æ¢³ç†ä¸€éï¼Œä»¥æ™®é€šçš„ç¥ç»ç½‘ç»œä¸ºåˆ‡å…¥ç‚¹ï¼Œæ¢ä¸ªè§’åº¦ç ”ç©¶ä¸‹CNNã€‚

1. æ€»ä½“è€Œè¨€ï¼ŒCNNå’Œæ™®é€šçš„ç¥ç»ç½‘ç»œçš„åŒºåˆ«ï¼š

   > First of all, the layers are **organised in 3 dimensions**: width, height and depth. Further, the neurons in one layer do not connect to all the neurons in the next layer but only to a small region of it. Lastly, the final output will be reduced to a single vector of probability scores, organized along the depth dimension.

   ![Normal NN vs CNN.â€Šâ€”â€ŠSource: <http://cs231n.github.io/convolutional-networks/>](https://cdn-images-1.medium.com/max/1600/1*U8huw63urvRLUwJe89VXpA.png)

2. CNNä¸­ä¸€å…±æœ‰è¿™ä¹ˆå‡ ç§layerï¼š

   > - INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.
   > - CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.
   > - RELU layer will apply an elementwise activation function, such as the ***max(0,x)*** thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).
   > - POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].
   > - FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.

   INPUTå°±ä¸è¯´äº†ï¼ŒCONVæ˜¯å·ç§¯å±‚ï¼Œä¹Ÿæ˜¯CNNçš„æ ¸å¿ƒï¼Œå·ç§¯å±‚æœ€å¤§çš„ç‰¹ç‚¹ï¼ˆç›¸æ¯”äºä¸€èˆ¬çš„NNï¼‰æ˜¯å®ƒåªå…³æ³¨è¾“å…¥çš„å±€éƒ¨ä¿¡æ¯ï¼›RELUå±‚çš„ä½œç”¨åº”è¯¥å’Œæ™®é€šçš„NNå·®ä¸å¤šï¼šæ³¨å…¥ä¸€äº›éçº¿æ€§çš„å› ç´ ï¼›POOLæ˜¯æ± åŒ–å±‚ï¼Œä½œç”¨ç±»ä¼¼äºé™é‡‡æ ·ï¼Œé™é‡‡æ ·åå†è¾“å…¥åˆ°ä¸‹ä¸€å±‚å·ç§¯å±‚ï¼Œé‚£ä¹ˆæ›´å…¨å±€çš„ä¿¡æ¯å°±è¢«å…³æ³¨åˆ°äº†ï¼Œå¯ä»¥æœ‰æ•ˆçš„é™ä½overfittingï¼ˆè¯•æƒ³ä¸‹å¦‚æœåªç»™ä½ çœ‹æ ‘å¶çš„çº¹è·¯ï¼Œé¬¼èƒ½çœ‹å¾—å‡ºè¿™æ˜¯ä»€ä¹ˆæ ‘ï¼‰å’Œæé«˜robustï¼ˆå³ä½¿å›¾åƒæœ‰ä¸€äº›å¹³ç§»æˆ–æ—‹è½¬ï¼Œé™é‡‡æ ·åå¯èƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥æ²¡æœ‰å¤ªå¤§å½±å“ï¼‰ï¼›FCæ˜¯å…¨è¿æ¥å±‚ï¼Œå¯ä»¥ç†è§£ä¸ºå°±æ˜¯æŠŠä¹‹å‰å„ç§å·ç§¯ã€æ± åŒ–ç­‰æ“ä½œä¹‹åçš„è¾“å‡ºå±•å¼€ä¸ºä¸€ç»´çš„å‘é‡ï¼Œå†è¾“å…¥åˆ°äº†ä¸€ä¸ªæ™®é€šçš„ç¥ç»ç½‘ç»œä¸­ï¼ˆæ™®é€šçš„NNä¸­æ¯ä¸€å±‚éƒ½æ˜¯å…¨è¿æ¥çš„ï¼‰ã€‚

3. å…³äº[å·ç§¯](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF)ï¼šå·ç§¯å…¶å®å°±æ˜¯**åŠ æƒå åŠ **ï¼Œåœ¨å›¾åƒå¤„ç†ä¸­é€šå¸¸ä¼šé€‰ç”¨ä¸€ä¸ªè¾ƒå°çš„çŸ©é˜µæ¨¡æ¿ï¼ˆä¹Ÿå«æè¿°å­æˆ–å·ç§¯æ ¸ï¼Œæ¯”å¦‚ä¸€ä¸ª3x3çš„çŸ©é˜µï¼‰æ¥å¯¹å›¾åƒä¸­çš„æ¯ä¸€ä¸ªåƒç´ å’Œå…¶å‘¨å›´éƒ¨åˆ†åƒç´ è¿›è¡ŒåŠ æƒå åŠ è®¡ç®—ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå…¨æ–°çš„å›¾åƒï¼ˆå¯¹äºå›¾åƒè¾¹ç¼˜çš„åƒç´ ï¼Œä¸€èˆ¬ä¼šé‡‡å–é•œåƒæ“ä½œæ¥å¡«å……ä¸€äº›å›¾åƒå¤–çš„ç‚¹æ¥è®¡ç®—ï¼Œä»¥ä¿è¯å·ç§¯è¿‡åçš„å›¾åƒå°ºå¯¸ä¸å˜ï¼Œæ¯”å¦‚è¦åœ¨å›¾åƒ`[5,0]`å¤„å’Œ3x3çš„çŸ©é˜µè¿›è¡Œå·ç§¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¼šç”¨`[5,1]`å¤„çš„å›¾åƒåƒç´ æ¥å¡«å……åˆ°`[5,-1]`çš„ä½ç½®ï¼Œç„¶åå°±å¯ä»¥è¿›è¡Œå·ç§¯å•¦ï¼‰ã€‚å›¾åƒå·ç§¯åœ¨å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨éå¸¸å¹¿æ³›ï¼Œæ¯”å¦‚å›¾åƒçš„å»å™ªã€é”åŒ–ã€æ¨¡ç³ŠåŒ–ã€è¾¹ç¼˜æ£€æµ‹ç­‰ç­‰ï¼Œè¿™äº›æ“ä½œå…¶å®éƒ½æ˜¯åœ¨å›¾åƒä¸Šè¿›è¡Œå·ç§¯ï¼Œæ‰€ä¸åŒçš„æ˜¯å®ƒä»¬ä½¿ç”¨çš„å·ç§¯æ ¸ä¸åŒç½¢äº†ã€‚
   ![An example](https://pic4.zhimg.com/v2-15fea61b768f7561648dbea164fcb75f_b.gif)
   <!--break-->

4. å…³äºå·ç§¯ï¼Œæ¢ä¸ªè§’åº¦ï¼š

   > **å·ç§¯å®šç†**æŒ‡å‡ºï¼Œå‡½æ•°å·ç§¯çš„[å‚…é‡Œå¶å˜æ¢](https://zh.wikipedia.org/wiki/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2)æ˜¯å‡½æ•°å‚…é‡Œå¶å˜æ¢çš„ä¹˜ç§¯ã€‚å³ï¼Œä¸€ä¸ªåŸŸä¸­çš„å·ç§¯ç›¸å½“äºå¦ä¸€ä¸ªåŸŸä¸­çš„ä¹˜ç§¯ï¼Œä¾‹å¦‚[æ—¶åŸŸ](https://zh.wikipedia.org/wiki/%E6%99%82%E5%9F%9F)ä¸­çš„å·ç§¯å°±å¯¹åº”äº[é¢‘åŸŸ](https://zh.wikipedia.org/wiki/%E9%A2%91%E5%9F%9F)ä¸­çš„ä¹˜ç§¯ã€‚

   å› æ­¤å¯¹å›¾åƒçš„å±€éƒ¨åšå·ç§¯å°±ç›¸å½“äºåœ¨å›¾åƒé¢‘åŸŸä¸­åšä¹˜ç§¯ï¼Ÿï¼ˆæˆ‘å¯¹å›¾åƒé¢‘åŸŸè¿™å—ç†è§£ä¸æ·±ï¼Œåªè®°å¾—å›¾åƒé«˜é¢‘ä»£è¡¨äº†å›¾åƒçš„ä¸€äº›ç»†èŠ‚ç‰¹å¾ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ä½é€šæ»¤æ³¢å™¨æ¥å¯¹å›¾åƒè¿›è¡Œé™å™ªï¼ˆè¿‡æ»¤æ‰ç‰¹åˆ«é«˜é¢‘çš„éƒ¨åˆ†å¥½åƒä¹Ÿç±»ä¼¼äºå¯¹å›¾åƒåšäº†å¹³æ»‘å·ç§¯çš„æ“ä½œï¼Ÿï¼‰ï¼‰

5. æŠŠå·ç§¯çš„å…¬å¼å±•å¼€ï¼šè®¾3x3çš„å·ç§¯æ ¸Wï¼Œå…¶ä¸­Wijè¡¨ç¤ºå…¶ä¸­ç¬¬iè¡Œjåˆ—çš„å‚æ•°ï¼Œè¾“å…¥ä¸ºXï¼ŒåŒæ ·Xijè¡¨ç¤ºè¾“å…¥çš„ç¬¬iè¡Œjåˆ—çš„å€¼ï¼Œé‚£ä¹ˆå…¶ä¸­ä¸€ä¸ªå·ç§¯ç»“æœå¯ä»¥è¡¨ç¤ºä¸ºï¼š

   ```
   O22 = W11*X11 + W12*X12 + W13*X13 + W21*X21 + W22*X22 + W23*X23 + W31*X31 + W32*X32 + W33*X33
   ```

   æœ€ç»ˆçš„è¾“å‡ºè¿˜ä¼šå†åŠ ä¸Šä¸€ä¸ªåç½®bã€‚å¦‚æœæŠŠè¿™é‡Œçš„Xå’ŒWçœ‹åšä¸€ä¸ªä¸€ç»´çš„å‘é‡è€Œä¸æ˜¯äºŒç»´çŸ©é˜µï¼Œæ˜¯ä¸æ˜¯æ„Ÿè§‰çªç„¶ç†Ÿæ‚‰ï¼Ÿæ™®é€šçš„ç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒçš„ä¹Ÿæœ‰è¿™æ ·çš„è®¡ç®—ï¼Œåªä¸è¿‡å®ƒçš„è¾“å…¥æ˜¯ä¸Šä¸€å±‚çš„æ‰€æœ‰çš„è¾“å‡ºï¼Œè€Œè¿™é‡Œçš„è¾“å…¥åªé€‰å–äº†ä¸Šä¸€å±‚è¾“å‡ºçš„ä¸€éƒ¨åˆ†ã€‚æ‰€ä»¥ï¼Œè¿™å°±èƒ½ç†è§£ä¸ºå•¥CNNä¸­ç¥ç»å…ƒæ˜¯æŒ‰ç…§ä¸‰ç»´ç©ºé—´æ’åˆ—çš„ï¼Œæ¯ä¸€æ¬¡å·ç§¯è¿ç®—éƒ½å¯¹åº”äº†ä¸€ä¸ªç¥ç»å…ƒï¼Œå¹¶ä¸”å¯¹å·ç§¯å±‚è€Œè¨€è¿™äº›ç¥ç»å…ƒçš„æ¿€æ´»å‡½æ•°ä»æ˜¯çº¿æ€§çš„ï¼ˆy=xå˜›ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥è¯´æ˜¯æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼‰ã€‚

6. å…·ä½“å®ç°ä¸Šï¼ŒCNNï¼ˆä¸è€ƒè™‘å…¨è¿æ¥å±‚ï¼‰å’Œæ™®é€šçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆä»¥ä¸‹ç”¨DNNä»£æŒ‡ï¼‰çš„åŒºåˆ«åœ¨äºï¼š

   1. DNNçš„æ¯ä¸€å±‚çš„è¾“å‡ºä¸º1xNçš„å‘é‡ï¼›è€ŒCNNæ¯ä¸€å±‚çš„è¾“å‡ºä¸ºMxNxDçš„çŸ©é˜µï¼ˆå¯ä»¥çœ‹åšæ˜¯Dä¸ªMxNçš„äºŒç»´çŸ©é˜µï¼‰ã€‚
   2. DNNçš„æ¯ä¸€å±‚éƒ½åŒ…å«äº†å¾ˆå¤šçš„ç¥ç»å…ƒï¼Œè€Œæ¯ä¸ªç¥ç»å…ƒåˆ™å«æœ‰è‡ªå·±çš„æƒé‡ï¼ˆweightsï¼‰ï¼Œé€šè¿‡å¯¹æŸå¤±å‡½æ•°åå‘ä¼ æ’­ï¼Œæˆ‘ä»¬è®¡ç®—å‡ºæ¯ä¸ªç¥ç»å…ƒæƒé‡çš„æ¢¯åº¦ï¼Œè¿›è€Œä¿®æ”¹ç¥ç»å…ƒçš„æƒé‡æ¥é™ä½æŸå¤±å‡½æ•°çš„è¾“å‡ºï¼›è€ŒCNNçš„æ¯ä¸€å±‚ä¹Ÿæœ‰å¾ˆå¤šç¥ç»å…ƒï¼Œä½†å®ƒä»¬ä¼šå…±äº«åŒä¸€å¥—æƒé‡ï¼ˆä½¿ç”¨äº†ç›¸åŒçš„å·ç§¯æ ¸ï¼‰ï¼ŒåŒæ ·é€šè¿‡å¯¹æŸå¤±å‡½æ•°åå‘ä¼ æ’­ï¼Œæ¥ä¿®æ”¹ç¥ç»å…ƒå…±äº«çš„æƒé‡ï¼ˆå·ç§¯æ ¸ï¼‰ã€‚
   3. DNNçš„å±‚ä¸å±‚ä¹‹é—´æ˜¯å…¨è¿æ¥çš„ï¼Œå› æ­¤ç¥ç»å…ƒå¯ä»¥ä»»æ„æ’å¸ƒï¼›è€ŒCNNä¸­å·ç§¯æ“ä½œä½¿ç¥ç»å…ƒåªå’Œä¸Šä¸€å±‚çš„ä¸€éƒ¨åˆ†ç¥ç»å…ƒäº§ç”Ÿäº†è¿æ¥ï¼Œå¹¶ä¸”ç”±äºå·ç§¯æ˜¯å’Œç©ºé—´ä½ç½®ç›¸å…³çš„ï¼Œæ‰€ä»¥ç¥ç»å…ƒçš„ç©ºé—´æ’å¸ƒç›´æ¥å½±å“åˆ°ä¸‹ä¸€å±‚çš„è¾“å‡ºï¼Œå› æ­¤ä½ç½®ä¸èƒ½éšæ„æ”¹å˜ã€‚
   4. CNNä¸­çš„è¶…å‚æ•°ï¼ˆhyperparameterï¼‰å’ŒDNNä¸å¤ªä¸€æ ·ï¼šCNNä¸­å·ç§¯å±‚éœ€è¦ç¡®å®šå·ç§¯æ ¸çš„å°ºå¯¸ã€å·ç§¯æ ¸çš„æ•°ç›®ã€å·ç§¯æ ¸æ»‘åŠ¨çš„æ­¥å¹…ï¼ˆstrideï¼‰ã€è¾“å…¥è¾¹ç¼˜å¡«è¡¥çš„æ–¹å¼ï¼ˆè¡¥é›¶ã€é•œåƒç­‰ï¼‰ï¼Œæ± åŒ–å±‚éœ€è¦ç¡®å®šæ± åŒ–çš„å°ºåº¦ï¼›è€ŒDNNä¸­åˆ™éœ€è¦ç¡®å®šçš„æ˜¯ï¼Œæ¯ä¸€å±‚çš„ç¥ç»å…ƒæ•°ç›®å’Œç¥ç»å…ƒçš„æ¿€æ´»å‡½æ•°ã€‚å±‚æ•°ã€è®­ç»ƒè¿­ä»£æ¬¡æ•°ã€å­¦ä¹ ç‡å’ŒæŸå¤±å‡½æ•°æ˜¯ä¸¤è€…éƒ½å­˜åœ¨çš„è¶…å‚æ•°ã€‚
      ï¼ˆå…³äºå‚æ•°å’Œè¶…å‚æ•°ï¼šå‚æ•°æ˜¯éšç€è®­ç»ƒä¼šä¸æ–­æ”¹å˜çš„å˜é‡ï¼Œæ¯”å¦‚ç¥ç»å…ƒä¸­çš„æƒé‡å’Œåå·®ï¼Œè€Œè¶…å‚æ•°åˆ™æ˜¯ç”¨äºç¡®å®šæ¨¡å‹çš„ä¸€äº›å‚æ•°ï¼Œä¸ä¼šéšæ¨¡å‹è®­ç»ƒè€Œæ”¹å˜ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ã€è¿­ä»£æ¬¡æ•°ã€å±‚æ•°ã€æ¯å±‚ç¥ç»å…ƒçš„ä¸ªæ•°ç­‰ã€‚ï¼‰

7. å…³äºå‚æ•°å…±äº«ï¼ŒCNNä¹‹æ‰€ä»¥ç”¨åŒä¸€ä¸ªå·ç§¯æ ¸æ¥å¯¹è¾“å…¥è¿›è¡Œå·ç§¯æ“ä½œï¼ŒStandfordæ•™ç¨‹é‡Œæ˜¯è¿™ä¹ˆè¯´çš„ï¼š

   > It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2).

   > If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images.

   åªèƒ½è¯´æ˜¯ä¸€ç§ç›´è§‰å’ŒçŒœæƒ³å§ï¼Œä¸è¿‡ä¹Ÿç¡®å®è¯´å¾—é€šï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æœŸæœ›å°†å›¾åƒä¸­çš„æŸä¸ªç›®æ ‡å¹³ç§»åè¿˜èƒ½ç…§æ ·æ£€æŸ¥æˆ–è¯†åˆ«å‡ºæ¥çš„ï¼Œå¹³ç§»åç›®æ ‡åŒºåŸŸå°±æ˜¯å…¶ä»–çš„ç¥ç»å…ƒæ¥å¯¹å®ƒåšå·ç§¯äº†ï¼Œé‚£è¿™ä¸ªç¥ç»å…ƒç”±äºä½¿ç”¨äº†ç›¸åŒçš„å·ç§¯æ ¸ï¼Œå› æ­¤å¯ä»¥å’Œä¹‹å‰é‚£ä¸ªç¥ç»å…ƒå·ç§¯å¾—åˆ°ç›¸åŒçš„ç‰¹å¾ã€‚å¦å¤–ï¼Œä»è®¡ç®—æ€§èƒ½ä¸Šæ¥è€ƒè™‘ï¼Œå…±äº«å‚æ•°ä¹Ÿæ˜¯æœ‰å¿…è¦çš„ã€‚

   > Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a **Locally-Connected Layer**.

   æ„Ÿè§‰åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿˜ä½¿ç”¨å…±äº«å‚æ•°é‚£å¥—æ¡†æ¶ï¼Œä½†ä½¿ç”¨æ›´å¤šçš„ç¥ç»å…ƒï¼ˆå·ç§¯æ ¸ï¼‰å’Œæ›´å¤šçš„å±‚æ•°åº”è¯¥ä¹Ÿèƒ½è¾¾åˆ°ä¸€æ ·å¥½çš„æ•ˆæœï¼Œåªä¸è¿‡åœ¨ä¸åŒåŒºåŸŸæå–ä¸åŒçš„ç‰¹å¾å¯ä»¥èŠ‚çœç¥ç»å…ƒï¼ˆè®¡ç®—é‡ï¼‰ã€‚

8. ç»¼ä¸Šï¼ŒCNNå¯ä»¥çœ‹åšæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œçš„ç‰¹ä¾‹ï¼Œç‰¹æ®Šåœ¨äºç¥ç»å…ƒçš„å‚æ•°å…±äº«å’Œå±€éƒ¨è¿æ¥ã€‚ç‰¹æ®ŠåŒ–æ„å‘³ç€å®ƒå¯¹æŸä¸€ç±»é—®é¢˜ä¼šæ›´åŠ æœ‰æ•ˆï¼ˆç»Ÿè®¡å­¦ä¸Šæ¥è¯´å°±æ˜¯å‡å°‘äº†å‡è®¾ç©ºé—´ï¼Œä»è€Œä½¿è®­ç»ƒå¾—åˆ°ç†æƒ³æ¨¡å‹çš„æ¦‚ç‡æé«˜äº†ï¼‰ï¼Œä½†å¯¹äºè§£å†³å…¶ä»–çš„é—®é¢˜å¯èƒ½å®Œå…¨æ— æ•ˆã€‚æ¢å¥è¯è¯´ï¼Œä½¿ç”¨æ›´åŠ é€šç”¨çš„DNNç†è®ºä¸Šåº”è¯¥ä¹Ÿèƒ½å¤Ÿè§£å†³CNNèƒ½è§£å†³çš„é—®é¢˜ï¼Œä½†ä»£ä»·å¯èƒ½æ˜¯éœ€è¦æ›´å¤šçš„ç¥ç»å…ƒä»¥åŠå¤šå¾—å¤šå¾—å¤šçš„å‚æ•°è¦è®­ç»ƒã€‚

### Reference

[https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)

[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)

[https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc](https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc)